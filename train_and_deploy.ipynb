{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Title\n",
    "\n",
    "This notebook lists all the steps that you need to complete the complete this project. You will need to complete all the TODOs in this notebook as well as in the README and the two python scripts included with the starter code.\n",
    "\n",
    "\n",
    "**TODO**: Give a helpful introduction to what this notebook is for. Remember that comments, explanations and good documentation make your project informative and professional.\n",
    "\n",
    "**Note:** This notebook has a bunch of code and markdown cells with TODOs that you have to complete. These are meant to be helpful guidelines for you to finish your project while meeting the requirements in the project rubrics. Feel free to change the order of these the TODO's and use more than one TODO code cell to do all your tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install any packages that you might need\n",
    "!pip install smdebug --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "TODO: Explain what dataset you are using for this project. Maybe even give a small overview of the classes, class distributions etc that can help anyone not familiar with the dataset get a better understand of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to download and unzip data\n",
    "!wget https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
    "!unzip dogImages.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r .dogImages/train/.ipynb_checkpoints\n",
    "!rm -r .dogImages/valid/.ipynb_checkpoints\n",
    "!rm -r .dogImages/test/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = './dogImages'\n",
    "\n",
    "arr_train = []\n",
    "arr_valid = []\n",
    "arr_test = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(basedir):\n",
    "    for file in files:\n",
    "        path_arr = subdir.split('/')\n",
    "        folder = path_arr[2]\n",
    "        label = int(path_arr[3].split('.')[0])\n",
    "        if (folder == 'train'):\n",
    "            arr_train.append([label, subdir, file])\n",
    "        elif (folder == 'valid'):\n",
    "            arr_valid.append([label, subdir, file])\n",
    "        elif (folder == 'test'):\n",
    "            arr_test.append([label, subdir, file])\n",
    "\n",
    "df_train = pd.DataFrame(arr_train, columns = ['label', 'subdir', 'file']).sort_values(['label', 'file'], ignore_index = True)\n",
    "df_train['row'] = range(len(df_train))\n",
    "\n",
    "df_valid = pd.DataFrame(arr_valid, columns = ['label', 'subdir', 'file']).sort_values(['label', 'file'], ignore_index = True)\n",
    "df_valid['row'] = range(len(df_valid))\n",
    "\n",
    "df_test = pd.DataFrame(arr_test, columns = ['label', 'subdir', 'file']).sort_values(['label', 'file'], ignore_index = True)\n",
    "df_test['row'] = range(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of classes\n",
    "num_classes = df_train['label'].nunique()\n",
    "print(f'There are {num_classes} classes in this dataset.')\n",
    "\n",
    "# Examine a sample of rows of each dataframe\n",
    "print(f'There are {len(df_train.index)} training images.')\n",
    "df_train.sample(n = 10, random_state = 1)\n",
    "print(f'There are {len(df_valid.index)} validation images.')\n",
    "df_valid.sample(n = 10, random_state = 1)\n",
    "print(f'There are {len(df_test.index)} testing images.')\n",
    "df_test.sample(n = 10, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breed_labels = df_train[['label', 'subdir']].copy().drop_duplicates().rename(columns = {'subdir': 'breed'}).set_index('label')\n",
    "df_breed_labels['breed'] = df_breed_labels['breed'].apply(lambda row: row.split('/')[-1].split('.')[-1].replace('_', ' '))\n",
    "df_breed_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up variables related to AWS account\n",
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "print(f'Default Bucket: {bucket}')\n",
    "\n",
    "region = session.boto_region_name\n",
    "print(f'AWS Region: {region}')\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f'RoleArn: {role}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dog images to S3\n",
    "os.environ[\"DEFAULT_S3_BUCKET\"] = bucket\n",
    "!aws s3 sync ./dogImages/train/ s3://${DEFAULT_S3_BUCKET}/data/train/\n",
    "!aws s3 sync ./dogImages/valid/ s3://${DEFAULT_S3_BUCKET}/data/valid/\n",
    "!aws s3 sync ./dogImages/test/ s3://${DEFAULT_S3_BUCKET}/data/test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "**TODO:** This is the part where you will finetune a pretrained model with hyperparameter tuning. Remember that you have to tune a minimum of two hyperparameters. However you are encouraged to tune more. You are also encouraged to explain why you chose to tune those particular hyperparameters and the ranges.\n",
    "\n",
    "**Note:** You will need to use the `hpo.py` script to perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some prerequisite inputs\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner\n",
    ")\n",
    "\n",
    "# Declare hyperparameter ranges \n",
    "hyperparameter_ranges = {\n",
    "    'lr': ContinuousParameter(1e-4, 0.1),\n",
    "    'batch-size': CategoricalParameter([32, 64, 128, 256]),\n",
    "    'epochs': IntegerParameter(6, 10)\n",
    "}\n",
    "\n",
    "# Declare metrics for our model\n",
    "objective_metric_name = 'ValidationNumCorrect'\n",
    "objective_type = 'Maximize'\n",
    "metric_definitions = [\n",
    "    {\n",
    "        'Name': 'ValidationNumCorrect', \n",
    "        \"Regex\": 'Validation Accuracy: ([0-9]+)'\n",
    "    },\n",
    "    {\n",
    "        'Name': 'ValidationLoss', \n",
    "        \"Regex\": 'Validation Set - Average Loss: ([0-9\\\\.]+)'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an estimator and a corresponding hyperameter tuner\n",
    "estimator = PyTorch(\n",
    "    entry_point = 'hpo.py',\n",
    "    role = role,\n",
    "    py_version = 'py36',\n",
    "    framework_version = '1.8',\n",
    "    instance_count = 1,\n",
    "    hyperparameters = {\n",
    "        'num-classes': num_classes\n",
    "    },\n",
    "    instance_type = 'ml.g4dn.2xlarge'\n",
    ")\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs = 10,                    # Up to how many combinations of HP's we want to try in total\n",
    "    max_parallel_jobs = 2,            # Up to how many HPO jobs we want to run at once\n",
    "    objective_type = objective_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data channels\n",
    "data_channels = {\n",
    "    'train': f's3://{bucket}/data/train',\n",
    "    'valid': f's3://{bucket}/data/valid'\n",
    "}\n",
    "\n",
    "# Perform HPO using the tuner\n",
    "tuner.fit(data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the best estimator model from HPO\n",
    "best_estimator = tuner.best_estimator()\n",
    "\n",
    "# Display the hyperparameters of the best trained model\n",
    "tuned_hyperparameters = best_estimator.hyperparameters()\n",
    "tuned_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Profiling and Debugging\n",
    "TODO: Using the best hyperparameters, create and finetune a new model\n",
    "\n",
    "**Note:** You will need to use the `train_model.py` script to perform model profiling and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up debugging and profiling rules and hooks\n",
    "from sagemaker.debugger import Rule, rule_configs, DebuggerHookConfig, CollectionConfig\n",
    "from sagemaker.debugger import ProfilerRule, ProfilerConfig, FrameworkProfile\n",
    "from sagemaker.debugger import DetailedProfilingConfig, DataloaderProfilingConfig,PythonProfilingConfig\n",
    "\n",
    "rules = [\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),\n",
    "    Rule.sagemaker(\n",
    "        base_config = rule_configs.loss_not_decreasing(),\n",
    "        rule_parameters={'tensor_regex': 'CrossEntropyLoss_output_0'}\n",
    "    )\n",
    "]\n",
    "\n",
    "hook_config = DebuggerHookConfig(\n",
    "    collection_configs = [\n",
    "        CollectionConfig(\n",
    "            name = 'CrossEntropyLoss_output_0',\n",
    "            parameters = {\n",
    "                'include_regex': 'CrossEntropyLoss_output_0', \n",
    "                'train.save_interval': '25',\n",
    "                'train.start_step': '1',\n",
    "                'eval.save_interval': '5',\n",
    "                'eval.start_step': '1',\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis = 500, \n",
    "    framework_profile_params = FrameworkProfile(\n",
    "        num_steps = 10,\n",
    "        detailed_profiling_config = DetailedProfilingConfig(),\n",
    "        dataloader_profiling_config = DataloaderProfilingConfig(),\n",
    "        python_profiling_config = PythonProfilingConfig()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Perform some modifications on the hyperparameters to ensure compatibility with the model. This will perform the casting in place.\n",
    "\n",
    "tuned_hyperparameters = { hp: value for hp, value in tuned_hyperparameters.items() if hp in ['batch-size', 'epochs', 'lr', 'num-classes'] }\n",
    "tuned_hyperparameters['batch-size'] = int(re.findall('\\d+', tuned_hyperparameters['batch-size'])[0])\n",
    "tuned_hyperparameters['epochs'] = int(tuned_hyperparameters['epochs'])\n",
    "tuned_hyperparameters['lr'] = float(tuned_hyperparameters['lr'])\n",
    "tuned_hyperparameters['num-classes'] = int(tuned_hyperparameters['num-classes'])\n",
    "\n",
    "tuned_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an estimator with the hyperparameters from the best estimator\n",
    "new_estimator = PyTorch(\n",
    "    entry_point = 'train_model.py',\n",
    "    role = role,\n",
    "    py_version = 'py36',\n",
    "    framework_version = '1.8',\n",
    "    instance_count = 1,\n",
    "    hyperparameters = tuned_hyperparameters,\n",
    "    instance_type = 'ml.g4dn.xlarge',\n",
    "    rules = rules,\n",
    "    debugger_hook_config = hook_config,\n",
    "    profiler_config = profiler_config\n",
    ")\n",
    "\n",
    "# Fit the new estimator\n",
    "input_channels = {\n",
    "    'train': f's3://{bucket}/data/train',\n",
    "    'valid': f's3://{bucket}/data/valid',\n",
    "    'test': f's3://{bucket}/data/test'\n",
    "}\n",
    "\n",
    "new_estimator.fit(input_channels, wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = new_estimator.latest_training_job.name\n",
    "client = new_estimator.sagemaker_session.sagemaker_client\n",
    "description = client.describe_training_job(TrainingJobName = job_name)\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.trials import create_trial\n",
    "from smdebug.core.modes import ModeKeys\n",
    "\n",
    "trial = create_trial(new_estimator.latest_job_debugger_artifacts_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a list of tensor values/variables that we can observe\n",
    "trial.tensor_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code from class notes to plot the evolution of the Cross Entropy Loss.\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "\n",
    "def get_data(trial, tname, mode):\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps(mode = mode)\n",
    "    vals = []\n",
    "    for s in steps:\n",
    "        vals.append(tensor.value(s, mode=mode))\n",
    "    return steps, vals\n",
    "\n",
    "def plot_tensor(trial, tensor_name):\n",
    "    steps_train, vals_train = get_data(trial, tensor_name, mode = ModeKeys.TRAIN)\n",
    "    steps_eval, vals_eval = get_data(trial, tensor_name, mode = ModeKeys.EVAL)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    host = host_subplot(111)\n",
    "\n",
    "    par = host.twiny()\n",
    "\n",
    "    host.set_xlabel(\"Steps (TRAIN)\")\n",
    "    par.set_xlabel(\"Steps (EVAL)\")\n",
    "    host.set_ylabel(tensor_name)\n",
    "\n",
    "    (p1,) = host.plot(steps_train, vals_train, label=tensor_name)\n",
    "    (p2,) = par.plot(steps_eval, vals_eval, label=\"val_\" + tensor_name)\n",
    "    leg = plt.legend()\n",
    "\n",
    "    host.xaxis.get_label().set_color(p1.get_color())\n",
    "    leg.texts[0].set_color(p1.get_color())\n",
    "\n",
    "    par.xaxis.get_label().set_color(p2.get_color())\n",
    "    leg.texts[1].set_color(p2.get_color())\n",
    "\n",
    "    plt.ylabel(tensor_name)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tensor(trial, 'CrossEntropyLoss_output_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?  \n",
    "**TODO**: If not, suppose there was an error. What would that error look like and how would you have fixed it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob\n",
    "\n",
    "job = TrainingJob(job_name, region)\n",
    "job.wait_for_sys_profiling_data_to_be_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the profiler output\n",
    "from smdebug.profiler.analysis.notebook_utils.timeline_charts import TimelineCharts\n",
    "\n",
    "system_metrics_reader = job.get_systems_metrics_reader()\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "\n",
    "view_timeline_charts = TimelineCharts(\n",
    "    system_metrics_reader,\n",
    "    framework_metrics_reader = None,\n",
    "    select_dimensions = ['CPU', 'GPU'],\n",
    "    select_events = ['total'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_output_path = new_estimator.output_path + new_estimator.latest_training_job.job_name + \"/rule-output\"\n",
    "print(f\"You will find the profiler report in {rule_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls {rule_output_path} --recursive\n",
    "! aws s3 cp {rule_output_path} ./ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deploying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Deploy your model to an endpoint\n",
    "\n",
    "predictor=estimator.deploy() # TODO: Add your deployment configuration like instance type and number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run an prediction on the endpoint\n",
    "\n",
    "image = # TODO: Your code to load and preprocess image to send to endpoint for prediction\n",
    "response = predictor.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remember to shutdown/delete your endpoint once your work is done\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
